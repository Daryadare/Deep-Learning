{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float64) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float64) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for fcl1_W\n",
      "Gradient check passed!\n",
      "Checking gradient for fcl1_B\n",
      "Gradient check passed!\n",
      "Checking gradient for fcl2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for fcl2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for fcl1_W\n",
      "Gradient check passed!\n",
      "Checking gradient for fcl1_B\n",
      "Gradient check passed!\n",
      "Checking gradient for fcl2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for fcl2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302029, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301685, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301851, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302623, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302031, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301124, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302831, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301638, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302170, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302117, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302455, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302807, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302832, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302390, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303278, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301772, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302333, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302433, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.301941, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302173, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22ceead1d20>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.275985, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.347515, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264348, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287890, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253264, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265011, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212698, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.320777, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302624, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311032, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234316, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283390, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.353994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217454, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.378711, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256101, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309860, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264767, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276485, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274562, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.322051, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308033, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317549, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262630, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270323, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308261, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264990, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278636, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267083, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279901, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276510, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314133, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256404, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295822, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290051, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278750, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280567, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255044, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240982, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.315651, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.351801, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.319693, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295926, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.345443, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.310013, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.326544, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.255641, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.199259, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.164561, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.025661, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.883798, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.263931, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.662918, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.245522, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.943906, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.663953, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.042200, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.418303, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.519709, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.823944, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.022291, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.585656, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.937038, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.908215, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.233499, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.095108, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.718242, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.162756, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.015933, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.363952, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.842670, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.657567, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.388463, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.911120, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.693765, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 2.010436, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.115900, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.613706, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.738574, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.885192, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.474042, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.119274, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.672047, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.767860, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.478802, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.926619, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.337383, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.683186, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.660081, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.401881, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.635264, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.635617, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.622938, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.854111, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.556636, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.634732, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.076198, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.007559, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.531114, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.041336, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.022564, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.647379, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.369538, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.379903, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.986917, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.318972, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.283966, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.936269, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.196061, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.739352, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.634901, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.201189, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.582981, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.742143, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.293135, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.748190, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.976545, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.357876, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.992895, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.602046, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.602167, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.173057, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.312798, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.564551, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.961713, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.274091, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.304703, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.958926, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.377168, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.657193, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.389767, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.576803, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.556738, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.175409, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.322616, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.251510, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.060407, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.661861, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.227308, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.597969, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.887250, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.338688, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.842966, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.160604, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.347215, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.223119, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.436648, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.535320, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.249121, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.248964, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.549803, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.259687, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.097239, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.324286, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.315431, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.420746, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.443017, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.301618, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.533658, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.607993, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.239065, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.649177, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.174445, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.602848, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.363682, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.160640, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.197888, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.126413, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.008120, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.643235, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.592807, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.412599, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.212850, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.196541, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.280346, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.403204, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.229377, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.400568, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.391042, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.102808, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.445285, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.406323, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.219193, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.449330, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.304464, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.494726, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.327706, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.361608, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.460816, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.424061, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.375612, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.404416, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.100168, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.280295, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.476688, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 1.243662, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.476998, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 1.050903, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.477044, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.413762, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 2.419081, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.765110, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 2.325244, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 0.262037, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.577167, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.225678, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.149843, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.279294, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.303339, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.644891, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 150, reg = 1e-2)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=2)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
